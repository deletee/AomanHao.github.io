---
title: 机器学习-线性回归 
date: 2018-07-19 09:39:40
tags: [机器学习]
---

机器学习-线性回归 

<!--more-->
### LR模型
每个特征变量量可以⾸首先映射到⼀一个函数，然后再参与线性计算,模型如下：
$$y = \theta_0 + \theta_1x_1 + \theta_2x_2 + · · · + \theta_nx_n$$
其中$ x_1,x_2,...,x_n$表示自变量（特征分量），$y$表示因变量，$\theta$是权重，$\theta_0$是偏移项（截距）;$\theta_i$越大，说明$x_i$对$y$结果的影响越⼤
输入空间映射到特征空间(映射函数$\phi(x)$)，建模.为
$$ h_\theta(x)=\theta^T\phi(x)$$
特征映射相关技术，包括特征哈希、特征学习、Kernel等

---
### 目标函数 
预测值$ h_\theta(x)$与真实值$y$之差越小越好，加入损失函数:
$$J(\theta)={0.5}\sum_{i=1}^{n}{(h_\theta(x^i)-y^i)^2}$$
求$min{J(\theta)}$
损失函数就是$x^i$的预测值$h_\theta(x^i)$与真实值$y^i$之差的平方和

>回归模型（尤其是线性回归类）的⽬目标函数通常⽤用平⽅方损失函数来作为优化的⽬目标函数<br>

为什么用误差平方和作为目标函数：

>根据中⼼心极限定理理，把那些对结果影响⽐比较⼩小的变量量（假设独⽴立同分布）之和认为服从正态分布是合理理的
加入数据是高斯分布，
输入$x^i$，预测值$\theta^Tx^i$，真实值$y^i$，误差$\epsilon^{i}$
$$y^i=\theta^Tx^i+\epsilon^{i}$$
根据中心极限定理，认为变量之和服从高斯分布,即
$$\epsilon^{i} = y^i-\theta^Tx^i$$
则，x,y的条件概率为
$$p(y^i|x^i;\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^i-\theta^Tx^i)^2}{2\sigma^2})$$


---
例子
$$\begin{bmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\
\end{bmatrix}$$


