---
title: 面经—机器学习
date: 2018-07-16 09:39:40
tags: [C++, 机器学习, 笔面试]
---

面经—机器学习

<!--more-->

## CVTE面经
作者：一一后
链接：https://www.nowcoder.com/discuss/88069
来源：牛客网

1.解释方差

2.PCA的实现过程；推导PCA

3.传统的图像特征有哪些

4.Sift特征为什么能实现尺度不变性（讲sift原理到一半，我发现完全解释不了为啥尺度不变，就停了，尴尬）
[参考](https://blog.csdn.net/u014485485/article/details/78681086?locationNum=1&fps=1）
```
尺度不变性：
不管原图尺度是多少，在包含了所有尺度的尺度空间下都能找到那些稳定的极值点，这样就做到了尺度不变！
高斯函数是唯一可行的尺度空间核
```


5.Hough直线检测的原理

6.梯度下降和牛顿法的区别
```
牛顿法的优缺点
优点：二阶收敛，收敛速度快；
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢
梯度下降法的缺点：
靠近极小值时收敛速度减慢，；
直线搜索时可能会产生一些问题；
可能会“之字形”地下降。

牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快
```
7.SVM和Lr的共同点和不同点
```
LR和SVM都是分类算法
LR和SVM都是线性分类算法
LR和SVM都是监督学习算法
LR和SVM都是判别模型
LR和SVM在学术界和工业界都广为人知并且应用广泛

不同：
损失函数
LR：逻辑回归方法基于概率理论
逻辑回归考虑全局（远离的点对边界线的确定也起作用）
对数据不做处理
LR必须另外在损失函数上添加正则项

SVM：几何间隔最大化原理
支持向量机只考虑局部的边界线附近的点，线性SVM不直接依赖于数据分布
线性SVM依赖数据表达的距离测度，所以需要对数据先做归一化
SVM的损失函数就自带正则
```
8.rf和Adaboost的异同(优秀的基于决策树的组合算法)
```
1，相同：二者都是bootsrap自助法选取样本。 
2，相同：二者都是要训练很多棵决策树。 
3，不同：adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。 
4，不同：随机森林在训练每一棵树的时候，随机挑选了部分变量作为拆分变量，而不是所有的变量都去作为拆分变量。 
5，不同：在预测新数据时，adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值。
```
9.给出一堆大小不一的矩形框，快速求矩形框的灰度值之和（当时没理解，这不是肯定要遍历么…后来结束后我想这些矩形框可能是重叠的，估计是要问我关于Bing中快速求梯度的算法）

10.有什么要问他的

---
### 作业帮提前批机器学习算法岗
作者：编程一头牛
链接：https://www.nowcoder.com/discuss/90245
来源：牛客网

对数据预处理怎么填充的缺失值，哪些判定为异常值，对连续属性进行离散化有什么好处，Logistic回归能处理浮点数吗？多项式组合特征对哪个模型中效果提升最大。这个没答上来，问了面试官，面试官说是Logistic回归里面提升最大，而且组合起来的两个特征也是想出来的，没什么理论支撑。还问了模型的评价指标AUC是如何计算的，ROC曲线的横纵坐标代表了什么含义。XGBoost模型里面参数有哪些？如何发现过拟合。XGBoost模型中对数据进行采样的好处？

如何最快的找出两个集合中的交集，提出用哈希表的方法，问了这种方法的复杂度，然后又问如果这两个集合都特别大，不能再内存中构建哈希表该如何做？可能是想让我回答多线程相关的内容，但是我不会。如何设计哈希表？期间也问过有编过多线程多进程的代码吗

---
### 深信服【机器学习】一面
作者：Rnanprince
链接：https://www.nowcoder.com/discuss/87283
来源：牛客网

【机器学习】一面：
1.项目介绍，研究的最成功的地方，我以写的文章为例，涉及到的知识点就问
2.笔试的数组求和100怎么做的？
没抽到这个题，但是做过；接着我说做了查找重复字符串最大长度，深搜，过了就没想别的方法
一个数组，求最长的连续子序列的起始下标，当时没理解明白，其实有歧义，简单说了一下
3.自己的哪些方面的优点没有涉及到，介绍一下
提到了SVM和决策树，介绍一下什么情况下使用？
svm:
```
这个模型的优势是什么？

分类效果好；
可以有效地处理高维空间的数据；
可以有效地处理变量个数大于样本个数的数据；
只是使用了一部分子集来进行训练模型，所以SVM模型不需要太大的内存；
可以提高泛化能力；
无局部极小值问题；
他什么情况下表现最好？

数据的维度较高；
需要模型具有非常强的泛化能力；
样本数据量较小时；
解决非线性问题；
这个模型的缺点是什么？

无法处理大规模的数据集，因为该算法需要较长的训练时间；
无法有效地处理包含噪声太多的数据集；
SVM模型没有直接给出概率的估计值，而是利用交叉验证的方式估计，这种方式耗时较长；
对缺失数据非常敏感；
对于非线性问题，有时很难找到一个合适的核函数。
什么条件下它表现很差？

数据集的数据量过大；
数据集中的含有噪声；
数据集中的缺失较多的数据；
对算法的训练效率要求较高；
根据我们当前数据集的特点，为什么这个模型适合这个问题。 
该项目所提供的样本数据相对较少；
该问题是属于非线性问题；
数据集经过“独热编码”后，维度较高
```

决策树：
```
这个模型的优势是什么？

决策树易于实现和理解；
对于决策树，数据的准备工作一般比较简单；
能够同时处理多种数据类型
给定一个决策树模型，可以根据产生的决策树推出相应的逻辑表达式；
通过静态测试来对模型的表现进行评价；
在相对较短的时间内可以对大量的数据做出非常好的结果；
决策树可以很好地扩展到大型数据中，同时决策树的大小独立于数据库的大小；
计算复杂度相对较低，结果的输出易于理解，对部分的数据缺失不敏感。
他什么情况下表现最好？

实例是由“属性-值”对表示的；
目标函数具有离散的输出值；
训练数据集包含部分错误(决策树对错误有适应性)；
训练数据缺少少量属性的实例。
这个模型的缺点是什么？

易于出现过拟合问题；
忽略了数据集中属性之间的相关性；
对于类比不一致的样本，决策树的信息增益倾向于那些数据值较多的特征
什么条件下它表现很差？

决策树匹配的数据过多时；
分类的类别过于复杂；
数据的属性之间具有非常强的关联。
根据我们当前数据集的特点，为什么这个模型适合这个问题。

不需要准备太多的训练数据，不需要对数据过多的处理如删除空白值等；
易于编码；
该问题是非线性问题，决策树能够很好地解决非线性问题；
算法的执行效率高，对机器的要求较小。
```


---
### 360浏览器事业部 推荐算法工程师
作者：泡了个泡
链接：https://www.nowcoder.com/discuss/77924
来源：牛客网

二面

1.项目

2.SVM原始问题为什么要转化为对偶问题，为什么对偶问题就好求解，原始问题不能求解么

3.K-means 中我想聚成100类 结果发现只能聚成98类，为什么

4.进程中的内存分段是怎样的

5.每个线程有哪些东西是自己独享的

6.一枚不均匀的硬币，我抛了100次，有70次朝上，那么第101次朝上的概率是多少

这个概率怎么样，公示是如何推导出来的

7.给你个字符串，字符串是个数字，怎么转换为int型，不用库函数的话

8.4个海盗，100个金币，每个人轮流提方案，如果你的方案有半数以上通过，那么久可以，否则就会被杀掉，如果你是第一个人，那么你怎么提方案比较好

9.你的优点是什么

### 腾讯沈阳现场一面

1.项目

2.特征选择方法都有用过哪些

3.随机森林怎么进行特征选择

4.用过哪些机器学习算法

5.加密方法知道哪些

6.MD5可逆么

7.word2vec用过么

8.极大似然估计是什么意思

9.上过哪些课

10.排序算法哪些时间复杂度比较低

11.计算机网络了解多少

### 阿里 新零售 天猫 算法工程师-机器学习
一面
先是一个简单的自我介绍；
1.然后介绍了项目的框架和主要创新点；

2.说一下随机森林和Adaboost，以及区别

3.说一下GBDT和Adaboost，以及区别

4.说一下LDA的原理

5.对于PCA，会有第一主成分、第二主成分，怎么为什么第一主成分是第一，原因是什么？

6.PCA的主成分是怎么得到的

3.面向对象的三要素

4.对深度学习了解多少

5.你觉得深度学习的方法和传统机器学习比，有什么大的优势

----

### 腾讯提前批
作者：IamBright
链接：https://www.nowcoder.com/discuss/75166
来源：牛客网

女朋友在广州又不想换工作的情况下，微信的机器学习算法工程师是最适合我实习的岗位了，因此最先让腾讯的同学内推了一波，在基本没有准备的情况下，接到了提前批电话一面二面，毫无意外的挂了。

电话一面
聊论文，但多数听我在说，没插话什么问题。最后问我第二篇论文里RNN实现的时候有什么trick。
问了问凸优化了解吗？传统机器学习了解吗？我答机器学习基本知识都学过，凸优化只了解和机器学习优化算法相关的。也没有继续问细节了。
编程题：打印所有子集，我用了迭代，但是写的比较蠢，好在不用调试运行
电话二面
聊论文，最后问了我跟什么算法做了对比，问我研究的实际意义，产业界现在的水平
编程题：打印螺旋矩阵，要我给一个可运行的结果。很简单的题，我一个符号错误调了很久都没发现，这里应该就印象很差了。
linux里查看端口被占用的命令，linux不熟，没答上。
AUC是什么？我说了是ROC曲线下面积，但是想不起来ROC是啥。我都是做序列数据，没做过二分类问题。
LR和SVM的区别。我说了损失函数不同，然后说了SVM通过核技巧可以更好的应对非线性，但是前面好差，这里也没好好组织语言了。
提前批挂的没什么话说，就是没准备，好久没做过算法题的情况下，突然出题做就很不顺手。而且机器学习的基础知识都有点忘了，像AUC这种没用过的，基本一问就懵逼。

之后跟工作的同学聊了一下，来牛客刷了刷面经，制定了简单的复习内容和刷题计划。花了一周时间，复习了一下西瓜书前11章和deep learning book前11章，刷了leetcode上三四十道medium的题吧（链表、字符串、迭代、dfs、堆、树、动态规划等每天刷一类题练练手），并且给自己做完2篇论文都准备了面试介绍版，又让同学推了阿里和网易，并进入腾讯笔试流程。

---

### 算法、数据挖掘岗面经

作者：胖胖胖子
链接：https://www.nowcoder.com/discuss/81814
来源：牛客网

1.华为
回国之后参加的第一次面试就是华为的留学生专场招聘，岗位是大数据开放（华为好像填什么岗位都没差吧~），一共只有两面
一面：介绍我的项目经历，我介绍完面试官尴尬的笑了笑说他不是做大数据方向的，也就没问我什么，聊了聊天愉快的过了
二面：可能会针对笔试提问，因为面试官问我为什么没参加笔试，然后就问了问职业规划，说大数据方向都在深圳工作，然后就一直问怎么看待华为的加班文化，怎么看待压力之类的
大概10天之后就直接把offer发到了邮箱里
2.第四范式（二面挂）
刚面完华为就参加了第四范式的面试，以为面试就是聊聊天，后来发现真是naive，第四范式的岗位是nlp研究员
一面：就写了两个代码，一个是逆时针打印矩阵，一个是leetcode原题count and say，虽然当时还没刷过题但写的都是easy难度，就过了
二面：二面面试官是牛津大学的phd，问我是不是distinction我说不是，就感受到了一股失望~讲了讲项目，问的很细，但是都在自己的项目范围内，问完之后又是写代码，一个字符串如何删除不匹配的括号然后输出括号匹配的字符串，比如（abc（），输出（abc）和abc（），哼哼唧唧没写出来就跪了
3.招银网络
招银网络面试岗位是算法工程师，一共两轮技术面，一轮hr面
一面：讲项目，问了决策树ID3，C4.5，CART的区别，讲了SVM的原理，然后写了找两个数的最小公倍数的代码
二面：讲项目，然后面试官说他是做C++开发的，问我会不会C++，我说学过但很久没用了，就聊了聊天愉快的过了
hr面：略（就谈人生谈理想接不接受调岗）
4. 平安科技
招银网络面试岗位是算法工程师，一共两轮技术面，一轮hr面
一面：讲项目，问了问当时爬虫有没有用什么框架，然后问为什么文本分类我选择了朴素贝叶斯，然后让我说了一下对word2vec的理解
二面：最痛的一次面试，三个面试官，一个问数据结构和数据库：问我知道的数据结构和使用场景，然后口述怎么把单链表变双向链表，讲了一下红黑树，以及红黑树的应用，数据的范式，索引失效的情况，如何优化数据库性能等等。第二个面试官问操作系统和计算机网络：TCP三次握手，四次挥手，第二次挥手和第三次挥手的时间间隔如何界定，TCP拥塞控制，OSI五层模型，每层有什么协议，TCP和UDP的区别，UDP的应用（这个问题我没太理解），ARP协议，python多线程，python3对python2是否有改进。第三个面试官问设计模式和算法：python闭包，bagging/boosting的区别，XGboost特点，tf-idf缺点，单例模式（设计模式我完全不会），怎样设计一个分布式的爬虫。
hr面：大概聊了聊对薪资的期望，和工作的部门
5.链家
链家是参加了牛客的留学生专场，感觉链家的面试官人真的好~岗位是机器学习/数据挖掘工程师，不过链家面试有点坎坷面完前两面才发现我面的大数据开发，其实我想去的是数据挖掘，然后就开始了第三轮面试
一面：先写了一个很简单的判断两个二叉树是不是一样的树，然后就说给一台机器内存有限制，然后10台服务器，每个服务器上有一个1G文件，假设文件里单词，如何给这些单词按频率排序，又问了求数据流的中位数，最后是智力题，给你两个一模一样的杯子，假设一共有10层楼，怎样找到杯子摔下来能不碎的最高楼层。
二面：面试官先问我职业规划，我说数据挖掘，他就比较懵说他们是大数据开发部门，问我考不考虑，我说还是倾向于做数据挖掘，就开始了面试，问了数据库索引的优缺点，索引失效的情况，然后复合索引如何引用会失效，然后问了ACID，剩下的记不清了，说我基础不错去给我联系数据挖掘的面试官
三面：手写LR损失函数，LR/SVM区别，手画word2vec网络结构，bagging，boosting，stacking的区别和联系，如果RF和GBDT达到同样的准确度哪个分裂的树更少为什么，随机梯度下降和梯度下降哪一个更快，XGBoost特点，XGBoost的L1和L2正则化怎么体现，特征工程相关的卡方统计和互信息计算公式，然后写了一个如何用最少的硬币找钱。
HR面：介绍了一下链家的福利和部门的发展。
6.快手
快手4.18的笔试，5.5面试大概是备胎池捞起来了，三轮技术面，一轮hr面
一面：介绍项目，边讲项目边提问，然后写了个代码，判断有向图中是否有环
二面：介绍项目，边讲项目边提问，问了文本分类问什么朴素贝叶斯比SVM，决策树效果更好，然后分类性能度量，precision，recall，F1 score和ROC AUC对比，写了一个leetcode的flatten nested list iterator
三面：只写代码，第一个AABB的字符串输出AB，第二个找数组的最长递增子序列，然后介绍了一下他们做的东西
HR面：谈了谈offer